{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter \n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PATH:\n",
    "    lm_path = \"path for mpt-7b\"\n",
    "    lm_tokenizer_path = \"path for mpt-7b\"\n",
    "    checkpoint_path = \"path for openflamingo v2 checkpoint.pt\"\n",
    "args = PATH()\n",
    "device_set = 'cuda:0'\n",
    "device = torch.device(device_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flamingo,image_processor,tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path = 'ViT-L-14',\n",
    "    clip_vision_encoder_pretrained = \"openai\",\n",
    "    lang_encoder_path = args.lm_path,\n",
    "    tokenizer_path = args.lm_tokenizer_path,\n",
    "    cross_attn_every_n_layers=4,\n",
    "    # new params\n",
    "    inference=True,\n",
    "    precision ='fp16',\n",
    "    device = device_set,\n",
    "    checkpoint_path = args.checkpoint_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step 1: Setting and Loading images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "# demo_image_one = Image.open(\"test-006.jpg\")\n",
    "# demo_image_two = Image.open(\n",
    "#     requests.get(\n",
    "#         \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "#         stream=True\n",
    "#     ).raw\n",
    "# )\n",
    "demo_image_two = Image.open(\"test-006.jpg\")\n",
    "query_image = Image.open(\"test-006.jpg\")\n",
    "\n",
    "# query_image=query_image.filter(ImageFilter.GaussianBlur(radius=11))  \n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "# lang_x = tokenizer(\n",
    "#     [\"<image>Question:How many is the cats in the image? Short answer:two.<|endofchunk|><image>Question:What color is the wall? Short answer:white.<|endofchunk|><image>Question:How many is the oranges in the image? Short answer:\"],\n",
    "#     return_tensors=\"pt\",\n",
    "# )\n",
    "# add the demonstrations question and answer.\n",
    "lang_x = tokenizer(\n",
    "    [\"<image>Question: What kind of animals in the image? Answer: Dog. <|endofchunk|><image>Question: What kind of animals in the image? Answer: Dog. <|endofchunk|><image>Question: What kind of animals in the image? Answer:\"],\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Preprocessing and Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "# load data to gpus\n",
    "vision_x = vision_x.to(device).half()\n",
    "print(vision_x.device)\n",
    "input_ids=lang_x[\"input_ids\"]\n",
    "attention_mask = lang_x[\"attention_mask\"]\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "generated_text = flamingo.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=20,\n",
    "    num_beams=3,\n",
    ")\n",
    "print(tokenizer.decode(generated_text[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "of2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
